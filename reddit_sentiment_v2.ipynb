{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reddit Sentiment Analysis**\n",
        "\n",
        "*Written by Adithya Solai*\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "A script that fetches top Reddit posts from subreddits that match keywords, and then applies Sentiment Analysis on the comments section of each of those posts to calculate a Positivity & Negativity Score for each post. The keywords and subreddits to query are specified by the user.\n",
        "\n",
        "The VADER Sentiment Analyzer framework (https://www.nltk.org/_modules/nltk/sentiment/vader.html) is used to fetch a compound positivity/negativity score for each word in each comment of each post. Various normalization procedures are applied on these compound scores to enable comparisons between posts. After normalizations, the positive compound scores in each post's comments are summed to create an overall Positivity score for the post, and the same is done with negative compound scores for an overall Negativity score. The Positivity & Negativity scores for each post are then visualized.\n",
        "\n",
        "Body text of posts are not considered for this analysis because many popular Reddit posts are simply URL links to videos, which don't have any meaningful text to analyze. Only top-level comments are considered for analysis, which means replies to comments are not considered."
      ],
      "metadata": {
        "id": "ApA-jaubfUaH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DMs5UsCXM3_"
      },
      "source": [
        "## **Pre-Requisite Packages & Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbhXULQe1oo",
        "outputId": "37700593-15df-4203-82ef-483dc1bfc1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.5.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.7/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (1.2.3)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: en_core_web_sm in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (2021.10.8)\n",
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy \\\n",
        "&& pip install pandas \\\n",
        "&& pip install praw \\\n",
        "&& pip install nltk \\\n",
        "&& pip install emoji \\\n",
        "&& pip install en_core_web_sm \\\n",
        "&& pip install -U matplotlib==3.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yuOt9DmrQ3U",
        "outputId": "46eb90bc-6131-437c-f4c5-663c2869e743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import praw # PRAW API (https://praw.readthedocs.io/en/latest/index.html) is used to fetch comment data from Reddit.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import time\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "# sentiment analysis libraries\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "# Data pre-processing steps were taken from this tutorial: https://levelup.gitconnected.com/reddit-sentiment-analysis-with-python-c13062b862f6\n",
        "# data pre-processing libraries\n",
        "from datetime import datetime\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer # tokenizes comments into individual words\n",
        "from nltk.stem import WordNetLemmatizer # lemmatizes words to capture only the root of each word\n",
        "from nltk import FreqDist\n",
        "import emoji # to remove emojis\n",
        "import re # to remove links\n",
        "import en_core_web_sm # to get stopwords\n",
        "\n",
        "# visualization libraries\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTpqkmP-ILKL"
      },
      "source": [
        "## **Load Reddit Sentiment Analysis Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZdiAF24uRNM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To use the Reddit PRAW API, you must first make a Reddit App.\n",
        "\n",
        "Then, find your App at the bottom of this page: https://www.reddit.com/prefs/apps\n",
        "Click \"Edit\" to find your client_id under the name of your App, and your App secret.\n",
        "\n",
        "We also need a User Agent string that identifies our Web Browser type to \n",
        "send HTTPS requests to the Reddit API servers. This can easily be found here:\n",
        "https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
        "'''\n",
        "APEXFUND_REDDIT_APP_ID='App ID Here'\n",
        "APEXFUND_REDDIT_APP_SECRET='App Secret Here'\n",
        "MY_USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36'\n",
        "\n",
        "# initialize PRAW Reddit API client\n",
        "# The client should be not be in an asynchronous environment. Our use case is an\n",
        "# isolated script that runs. Asynchronous clients are best suited for integrating\n",
        "# Reddit API functionality into a web application or bot (to automatically\n",
        "# write Reddit Posts or do some other action under the authentication of\n",
        "# an individual user, for example). Our use case is read-only.\n",
        "# There is also a variation of the PRAW API that is meant only for aynchronous use cases: https://asyncpraw.readthedocs.io/en/stable/\n",
        "r = praw.Reddit(client_id=APEXFUND_REDDIT_APP_ID, client_secret=APEXFUND_REDDIT_APP_SECRET, user_agent=MY_USER_AGENT, check_for_async=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4JauHeTIuV6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Fetch Top 30 Reddit posts for the given search query in the given subreddit.\n",
        "# For each post, fetch its Post ID, the timestamp of its creation, and its URL.\n",
        "# Organize the details of each post into its own dictionary, and store those\n",
        "# dictionaries into a list. (The list should have 30 elements, and each element\n",
        "# is a post's details arranged into a dictionary)\n",
        "'''\n",
        "NUM_POSTS_TO_FETCH_LIMIT=30\n",
        "def fetch_posts(subreddit, query):\n",
        "  subreddit_to_search = r.subreddit(subreddit)\n",
        "\n",
        "  posts = []\n",
        "\n",
        "  for post in subreddit_to_search.search(query=query, sort='top', time_filter='year', limit=NUM_POSTS_TO_FETCH_LIMIT):\n",
        "    post_dict = {}\n",
        "    post_dict['id'] = post.id\n",
        "    post_dict['timestamp'] = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    post_dict['url'] = post.url\n",
        "    posts.append(post_dict)\n",
        "\n",
        "  return posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7JnA3OcTcrB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The input to this function is the output of fetch_posts() above --\n",
        "a list of the details for all 30 posts, where each element is a\n",
        "dictionary with the post's details.\n",
        "\n",
        "This function extracts the comments of each post, and adds\n",
        "those comments to each post's detail dictionary.\n",
        "'''\n",
        "def extract_post_comments(posts):\n",
        "  for post in posts:\n",
        "    post_id = post['id']\n",
        "    post['comments'] = extract_comments(post_id)\n",
        "\n",
        "  return posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbMMVP_iSArU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A function that extracts all top-layer comments for a given Reddit post ID,\n",
        "and puts the text, upvotes, and award coin value of each comment\n",
        "into a Pandas df.\n",
        "\n",
        "This function facilitates the extract_post_comments() function above.\n",
        "\n",
        "By default, this function will only fetch the default max amount of top-layer\n",
        "comments returned by the PRAW API, which is about 400 comments.\n",
        "If the `expand_top_layer` input is True, then more top-layer comments\n",
        "will be fetched, but this takes more time so it is left False by default.\n",
        "\n",
        "This function does not support expanding into the second layer of comments and\n",
        "below (meaning replies to top-layer comments, and replies to those replies).\n",
        "'''\n",
        "def extract_comments(submission_id, expand_top_layer=False):\n",
        "  post = r.submission(id=submission_id)\n",
        "\n",
        "  # Retrieve PRAW CommentForest object for the comments section (https://praw.readthedocs.io/en/stable/code_overview/other/commentforest.html)\n",
        "  # A CommentForest is a list of Comment and MoreComments objects.\n",
        "  # Comment objects are individual comments. By default, a CommentForest object only has a maximum of about 400 Comment objects\n",
        "  # MoreComments objects are a pointer to a list of more individual Comment objects (this saves RAM memory space when fetching CommentForest objects)\n",
        "  comment_forest = post.comments\n",
        "\n",
        "  # Extract only individual Comment objects from CommentForest\n",
        "  comment_objects = [comment for comment in comment_forest.list() if isinstance(comment, praw.models.reddit.comment.Comment)]\n",
        "\n",
        "  # Expand MoreComments objects to get more top-level comments\n",
        "  # This will only expand MoreComments objects once. Expanding a MoreComments object\n",
        "  # can yield more MoreComments objects in addition to individual Comment objects,\n",
        "  # but we will not expand that new level of MoreComments objects.\n",
        "  if expand_top_layer:\n",
        "    # Extract MoreComments objects from CommentForest\n",
        "    more_comments_objects = [comment for comment in comment_forest.list() if isinstance(comment, praw.models.reddit.more.MoreComments)]\n",
        "\n",
        "    # Fetch new list of Comment & MoreComments objects from each existing MoreComments object.\n",
        "    for idx, more_comments in enumerate(more_comments_objects):\n",
        "      curr_comments = more_comments.comments()\n",
        "      comment_objects += curr_comments\n",
        "\n",
        "    # trim off new MoreComments objects, and only keep individual Comment objects\n",
        "    comment_objects = [comment for comment in comment_objects if isinstance(comment, praw.models.reddit.comment.Comment)]\n",
        "\n",
        "  # Extract text, upvotes, and total award coin value of comments into DataFrame\n",
        "  comments_df = pd.DataFrame([(str(comment.body), \n",
        "                               comment.score,\n",
        "                               len(comment.all_awardings),\n",
        "                               sum([award['coin_price'] for award in comment.all_awardings])) for comment in comment_objects],\n",
        "                             columns=['text', 'upvotes', 'num_awards', 'total_award_coin_value'])\n",
        "  return comments_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl7I5BWkW8qY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Find positivity and negativity scores for each post, and add that to each post's \n",
        "detail dictionary. Along the way, a df with each word's polarity scores from\n",
        "the VADER Sentiment Analyzer for each post's comment section will also be\n",
        "added to the post's detail dictionary.\n",
        "'''\n",
        "def find_pos_neg_scores(posts):\n",
        "  # pre-process post comments and find global max_upvotes and max_coin_value\n",
        "  # global max_upvotes and max_coin_value are needed to normalize positivity and\n",
        "  # negativity scores across posts so that score comparisons between posts are valid.\n",
        "  global_max_upvotes = 1\n",
        "  global_max_coin_value = 1\n",
        "  for post in posts:\n",
        "    post_id = post['id']\n",
        "    post_comments = post['comments']\n",
        "\n",
        "    # expand each comment into rows for each word in the comment with pre-processing tasks applied\n",
        "    post_cleaned_comments = pre_process_comment_by_comment(pd.DataFrame(post_comments))\n",
        "\n",
        "    # fetch compound polarity score (positivity/negaitivity) for each word in the post\n",
        "    post_polarity_scores_df = convert_cleaned_comments_to_polarity_df(post_cleaned_comments)\n",
        "\n",
        "    # ensure upvotes and total award coin value within each post are >=1 to make calculations\n",
        "    # of positivity and negativity scores easier. \n",
        "    # also retrieve each post's largest upvotes value and largest total award coin value\n",
        "    post_polarity_scores_df, curr_max_upvotes, curr_max_coin_value = fix_and_get_upvotes_and_coin_value(post_polarity_scores_df)\n",
        "\n",
        "    post['polarity_df'] = post_polarity_scores_df\n",
        "\n",
        "    # update global max upvotes & coin_value for future normalization across all posts\n",
        "    global_max_upvotes = max(global_max_upvotes, curr_max_upvotes)\n",
        "    global_max_coin_value = max(global_max_coin_value, curr_max_coin_value)\n",
        "\n",
        "  # calculate positivity and negativity score\n",
        "  for post in posts:\n",
        "    post_polarity_scores_df = post['polarity_df']\n",
        "\n",
        "    # use global largest upvotes and total award coin value to find overall positivity & negativity scores for each post\n",
        "    # that can be validly compared to the scores of other posts\n",
        "    post_macro_positivity_score, post_macro_negativity_score = calculate_pos_and_neg_scores(post_polarity_scores_df, global_max_upvotes, global_max_coin_value)\n",
        "\n",
        "    post['scores'] = [post_macro_positivity_score, post_macro_negativity_score]\n",
        "\n",
        "  return posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW6WGKSlUaRI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The input to this function is the Pandas df returned by extract_comments().\n",
        "The input df contains the text, upvotes, and total award coin value for each\n",
        "comment in a Reddit post.\n",
        "\n",
        "This function returns a list that essentially expands each row in the \n",
        "input df into more rows based on how many words were in the `text` column for the input df.\n",
        "\n",
        "For example, if the input df has a row with \"I love GameStop\" in the `text` column,\n",
        "that one row would be replaced by 3 new dictionaries in the output 2D-list: one dictionary\n",
        "for each word in the sentence. Each new dictionary will also have the identifying ID, upvotes, \n",
        "and total award coin value of the original comment.\n",
        "\n",
        "Each word in the output list will also have other data pre-processing procedures applied,\n",
        "like removing emojis & URLs, as well as lemmatizing the word to only capture the root word.\n",
        "\n",
        "If a row in the input df has words in the `text` column that are stopwords (words\n",
        "that don't add meaningful sentiment to a sentence--like \"and\" & \"the\"), then those\n",
        "words will not have a dictionary in the output list.\n",
        "'''\n",
        "\n",
        "def pre_process_comment_by_comment(comments_df):\n",
        "  # Re-name left-most column to \"comment_num\" so that the output 2D-list's words\n",
        "  # can be mapped to their original comment with this number\n",
        "  comments_df['comment_num'] = comments_df.index\n",
        "\n",
        "  # drop rows with any null values\n",
        "  comments_df.dropna()\n",
        "\n",
        "  # `text` column dtype still shows up as `object` instead of `str` because strings\n",
        "  # can have variable length, so Pandas stores them as pointers to ndarrays of characters\n",
        "  # (explained better here: https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object)\n",
        "  comments_df = comments_df.astype({'text' : 'str'})\n",
        "\n",
        "  ## Apply data cleaning procedures to each comment (removing emojis, stopwords, and URLs, & lemmatizing) ##\n",
        "  \n",
        "  # Put each comment's text into a str list to make pre-processing easier,\n",
        "  # but keep upvotes, total award coin value, and comment_num attached to each word always.\n",
        "  comments_df_as_list = comments_df.loc[:, ['comment_num', 'text', 'upvotes', 'total_award_coin_value']].values.tolist()\n",
        "\n",
        "  # prepare output list\n",
        "  cleaned_words = []\n",
        "\n",
        "  # prepare tokenizer\n",
        "  tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
        "\n",
        "  # fetch stopwords\n",
        "  nlp = en_core_web_sm.load()\n",
        "  all_stopwords = nlp.Defaults.stop_words # this is a set, so it is O(1) to check if a word is a stopword\n",
        "\n",
        "  # prepare lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # apply text pre-processing on each comment\n",
        "  for row in comments_df_as_list:\n",
        "    comment_num = row[0]\n",
        "    words = row[1]\n",
        "    upvotes = row[2]\n",
        "    coin_value = row[3]\n",
        "\n",
        "    # remove emojis from comment text\n",
        "    words = emoji.get_emoji_regexp().sub(u'', words)\n",
        "\n",
        "    # tokenize comment text\n",
        "    words = tokenizer.tokenize(words)\n",
        "\n",
        "    # comment-level pre-processing tasks are done (like removing emojis and tokenizing)\n",
        "    # now, apply word-level pre-processing and append to output list\n",
        "    for word in words:\n",
        "      # convert word to lower-case first\n",
        "      word = word.lower()\n",
        "\n",
        "      # only add to output list if word is not a stopword\n",
        "      if word not in all_stopwords:\n",
        "        # lemmatize word before adding to output 2D list\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        cleaned_words.append({\"word\": word, \"comment_num\": comment_num, \"upvotes\": upvotes, \"coin_value\": coin_value})\n",
        "\n",
        "  return cleaned_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cn7Rz27X7Ky"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The input to this function is the output of the pre_process_comment_by_comment()\n",
        "function above--a list of dictionaries where each dictionary corresponds to a\n",
        "word in a post, and the word's comment's upvotes and total award coin value are\n",
        "also stored in the dictionary.\n",
        "\n",
        "This function uses the VADER Sentiment Analyzer to fetch a compound\n",
        "polarity score for each word in the input list. \n",
        "\n",
        "Compound polarity scores are between -1 (most negative) and +1 (most positive).\n",
        "If the compound polarity score is:\n",
        "- positive: then the word has positive sentiment \n",
        "- negative: then the word has negative sentiment\n",
        "- zero: then the word has neutral sentiment\n",
        "'''\n",
        "def convert_cleaned_comments_to_polarity_df(clean_comments_data):\n",
        "  #### Apply VADER Sentiment Analyzer to words from cleaned comments ####\n",
        "  sia = SIA()\n",
        "  results=[]\n",
        "\n",
        "  for word_dict in clean_comments_data:\n",
        "    word = word_dict['word']\n",
        "    comment_num = word_dict['comment_num']\n",
        "    upvotes = word_dict['upvotes']\n",
        "    coin_value = word_dict['coin_value']\n",
        "    \n",
        "    # fetch polarity score for the current word\n",
        "    # `pol_score` is already a key-value store dictionary with\n",
        "    # the compound polarity score and other unused sentiment scores\n",
        "    pol_score = sia.polarity_scores(word)\n",
        "\n",
        "    # append new key-value entries to label these polarity scores\n",
        "    # as belonging to the current word\n",
        "    pol_score['word'] = word\n",
        "    pol_score['comment_num'] = comment_num\n",
        "    pol_score['upvotes'] = upvotes\n",
        "    pol_score['coin_value'] = coin_value\n",
        "\n",
        "    results.append(pol_score)\n",
        "\n",
        "  # convert the list of dictionaries with word + polarity score info into a df\n",
        "  polarity_scores_df = pd.DataFrame.from_records(results)\n",
        "\n",
        "  return polarity_scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uylkCmFtXo_6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The input to this function is the output df of the convert_cleaned_comments_to_polarity_df()\n",
        "function above.\n",
        "\n",
        "This function modifies the upvotes values and total award coin value of each comment\n",
        "in this post so that they are all >= 1. Negative upvotes are possible for comments on Reddit,\n",
        "so this is necessary. Also, allowing upvotes or total award coin value to be 0 can\n",
        "cause issues with calculating normalized positivity & negativity scores later.\n",
        "\n",
        "This function returns the modified df, as well as the largest upvotes value\n",
        "and largest total award coin value for this post so that a global max\n",
        "can be calculated for these values across all posts for further normalization.\n",
        "'''\n",
        "def fix_and_get_upvotes_and_coin_value(polarity_scores_df):\n",
        "  # add all upvotes values by the min upvote value to make sure all upvotes are >= 0\n",
        "  polarity_scores_df['upvotes'] = polarity_scores_df['upvotes'] + abs(polarity_scores_df['upvotes'].min())\n",
        "\n",
        "  # add 1 to all upvotes to ensure all upvotes are >= 1 to avoid losing positivity/negativity\n",
        "  # on words that have 0 upvotes when these upvote values are multiplied with compound polarity\n",
        "  # scores to weight comments based on their relative upvotes\n",
        "  polarity_scores_df['upvotes'] = polarity_scores_df['upvotes'] + 1\n",
        "  # Do the same with award coin_value for the same reasons (we will also multiply compound polarity\n",
        "  # scores with total award coin value for further weighting)\n",
        "  polarity_scores_df['coin_value'] = polarity_scores_df['coin_value'] + 1\n",
        "\n",
        "  # fetch max upvotes\n",
        "  max_upvotes = polarity_scores_df['upvotes'].max()\n",
        "\n",
        "  # fetch max coin_value\n",
        "  max_coin_value = polarity_scores_df['coin_value'].max()\n",
        "\n",
        "  return polarity_scores_df, max_upvotes, max_coin_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LXRM7dVUphr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The `polarity_scores_df` input df to this function is the same as the output df of the\n",
        "fix_and_get_upvotes_and_coin_value() function above.\n",
        "\n",
        "In this function, the upvotes and total award coin value for each comment are first\n",
        "normalized (or divided) by the global maximum upvotes & total award coin value across \n",
        "all posts in this analysis. Then, the compound polarity scores for each word are weighted \n",
        "(or multiplied) by the new normalized upvotes and total award coin value. These two weighted\n",
        "compound polarity scores (one weighted by upvotes, one weighted by total award coin value)\n",
        "are summed to yield the \"Macro-Normalized Compound Polarity Score\" (or MNCPS) for each word.\n",
        "\n",
        "To calculate positivity score for this post, we simply sum all the positive MNCPS.\n",
        "To calculate negativity score for this post, we simply sum all the negative MNCPS.\n",
        "'''\n",
        "def calculate_pos_and_neg_scores(polarity_scores_df, global_max_upvotes, global_max_coin_value):\n",
        "  # create compound score that works across many posts. only multiply by upvotes & award coin value for now.\n",
        "  # then, we divide positivity/negativity scores by max upvotes & coin value across ALL posts\n",
        "  polarity_scores_df['compound_macro'] = polarity_scores_df['compound'] * polarity_scores_df['upvotes'] / global_max_upvotes\n",
        "  polarity_scores_df['compound_macro'] += polarity_scores_df['compound'] * polarity_scores_df['coin_value'] / global_max_coin_value\n",
        "\n",
        "  #### Calculate and return macro positivity & negativity scores ####\n",
        "  positivity_score_macro = polarity_scores_df.loc[polarity_scores_df['compound_macro'] > 0]['compound_macro'].sum()\n",
        "  negativity_score_macro = polarity_scores_df.loc[polarity_scores_df['compound_macro'] < 0]['compound_macro'].sum()\n",
        "\n",
        "  return (positivity_score_macro, negativity_score_macro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN8eyvwGri0E"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function uses the detail dictionary of each post to construct\n",
        "a double bar plot of each post's positivity and negativity score.\n",
        "The y-axis is the same units as the positivity and negativity scores,\n",
        "and the x-axis is the time at which the post was created.\n",
        "\n",
        "The original subreddit and keyword query are also taken as input \n",
        "for the title of the double bar plot.\n",
        "'''\n",
        "def plot_pos_neg_scores(posts, subreddit, query):\n",
        "  bar_chart_data = []\n",
        "  timestamps = []\n",
        "  positivity_scores = []\n",
        "  negativity_scores = []\n",
        "\n",
        "  url_data = []\n",
        "\n",
        "  for post in posts:\n",
        "    post_timestamp = post['timestamp']\n",
        "    post_scores = post['scores']\n",
        "    post_url = post['url']\n",
        "\n",
        "    # Use only date for cleaner graph\n",
        "    post_timestamp_as_date = int(datetime.strptime(post_timestamp, '%Y-%m-%d %H:%M:%S').month)\n",
        "\n",
        "    # Use abs() of negativity score to make bar chart cleaner\n",
        "    post_positivity_score = post_scores[0]\n",
        "    post_negativity_score = abs(post_scores[1])\n",
        "\n",
        "    bar_chart_data.append([post_timestamp_as_date, post_positivity_score, post_negativity_score])\n",
        "    url_data.append([post_timestamp_as_date, post_url])\n",
        "\n",
        "  bar_chart_data = np.array(bar_chart_data)\n",
        "\n",
        "  # sort by timestamp\n",
        "  bar_chart_data = bar_chart_data[bar_chart_data[:, 0].argsort()]\n",
        "\n",
        "  # Follow this documentation to make grouped bar chart:\n",
        "  # https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
        "\n",
        "  timestamps = bar_chart_data[:,0].astype('int')\n",
        "  positivity_scores = bar_chart_data[:,1]\n",
        "  negativity_scores = bar_chart_data[:,2]\n",
        "\n",
        "  # position timestamp as x-coordinate\n",
        "  x = np.arange(len(timestamps))  \n",
        "  width = 0.3  # the width of the bars\n",
        "\n",
        "  fig = plt.figure(figsize=(30,5))\n",
        "  ax = fig.add_subplot(121)\n",
        "\n",
        "  rects1 = ax.bar(x - width/2, positivity_scores, width, label='Positivity')\n",
        "  rects2 = ax.bar(x + width/2, negativity_scores, width, label='Negativity')\n",
        "\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_ylabel('Scores')\n",
        "  ax.set_xlabel('Month in 2021')\n",
        "  ax.set_title('Positivity & Negativity Scores for \"{0}\" Posts in r/{1} Over the Past Year'.format(query, subreddit))\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(timestamps)\n",
        "  ax.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Print URLs for each post in chronological order\n",
        "  print(\"Post URLs in Same Chronological Order as Chart Above:\")\n",
        "  url_data = pd.DataFrame(url_data, columns = ['Month', 'URL'])\n",
        "  url_data = url_data.sort_values('Month') # sort by timestamp\n",
        "\n",
        "  for index, row in url_data.iterrows():\n",
        "    print(\"Month: \", row['Month'], \"URL: \", row['URL'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1XBFiK6e2Co"
      },
      "source": [
        "## **Run Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "2Ygym-weeUbI",
        "outputId": "8d01a162-7405-42f0-9932-a8fd07e67635"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAykAAAFNCAYAAAAEkCAPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debgcZbWw/XuRBBJMGBMRDBKcgZAvSBA8iG8YxAioqEhEFAERFZFB3qMRzyvBc8SoKIoTgkBQGQU5IoNMEkARNdGITApIgDCGAEmYA1nfH1U7dDZ76CS7eldn37/r2teurmHV09VV3bXqeZ6qyEwkSZIkqS5W6+8CSJIkSVIjkxRJkiRJtWKSIkmSJKlWTFIkSZIk1YpJiiRJkqRaMUmRJEmSVCsmKVKTIuLoiPhpD9P3jYgrmox1S0RM7LPCtTm3XXMi4v0RcV9EPBkRW7VgfdOrXken9WVEvL6V6+xLVe+b5ef+2qriL6/lOW5XBRGxf0T8vr/LIQ0UJilaZUXEnIh4pvxhfzgipkfE8BWNl5nHZeZBZewx5QnV4IbpZ2bmrk3G2iIzZ5SxpkbEL1a0XGWMj5bvd2FE/CkiRvcy/9Sy/Hs3jBtcjhuzMmVpoqy12XYRsUVEXBERj0XEExExKyJ2W9F4LXA8cGhmDs/Mv61ssIiYERETy+04deWL13fK43X/8m96N/NMjIgl5TG+KCL+GREHrOR650TELiuybOO+2Vci4m0RcUMZf3hm/rsv4y9HOVbquF3Bde4fEf+IiKcj4qGI+HFErFPV+jqt+2Xvt4/jT42IxeW++0RE3BARb1vJmN0m+eV33YKIeGOn8VdHxLSVWa9UFZMUrerek5nDgbcAE4D/6ufy9Lky8TodOBhYBzgUeLaJRR8Djo2IQRUWr+5+A1wJvAp4JXAYsLAvV9DHJzmbALesYDma/pwj4ksRcQ/wkYiYGxHHrMg6W+SB8hhfC/gicEpEbN7PZVopnfaZ3YFL+6ss/SUijgK+AfwnsDawHcX+f2VErN7H66okEWnCueW+Owr4PfCriIgqVpSZt1Bc5Di1Yx0R8Qng1cDUvlhHFDyvVJ9xZ9KAkJn3A5cBYwEi4r1l04wnyqvJm3XMGxFfjIj7G67M7lyOb7xqf135/4nyStjbGpsClFf8jm8sQ0T8OiI+Xw7PiYhdImIScDQwuYzz94j4UETM6rTs5yPi1929PeAF4O7MXJKZf8nMR5vYLL8Fngc+2tXEiFgjIo6PiHujqIk6KSKGNUz/QkQ8GBEPRMRBjVfxImL3iPhbFDU793W6Sl+LbRcRI4FNgVMy8/ny7w+Z+fuGed4XEbPL93FXuU4iYqOIuCiKGpg7I+KTDctMjYjzI+IXEbEQ2D8i1o6IU8vtdX9E/E9H0hARr4+Ia8urnI9GxLndfBZPAoOAv0fEXeX4zcr994lyf35vwzLTy215aUQ8BezY1efcxbreRnFi+C7gLGAr4NqG6T19tkTEL6O46r0gIq6LiC06lemkiLiyPL6ujYhNmilXb7Lwv8DjwOblNvtuuX8+UA6vUZZjZERcXG63xyLi+ohYLSJ+DrwG+E25T30hIoaWn+X8cv6/RMQG3Wy7pbUw5X5wXkT8rHyvt0TEhO7KXx4/n42IO4A7GibtRpmkdDrGpkfEDyPikjL+nyLideW03o6hjSLigoiYFxF3R8RhDfO9NSJmlp/vwxHxnXJSj8dtQ/k+HRF3lNvqhxFLT4gHRcS3y3387og4NLqpqYiItYBjgc9l5m8zc3FmzgH2BsYAHy3fwzMRsV7DcluV8YeUrw+MiNsi4vGIuLxxX+the3d42fttWPb4MubdEfHuhvHdHuc9yczFwBkUF0vWj4gpUXzfLIqIWyPi/Q3r6PL7IiI6yvv3sryTu1jVNGAEcEi5D38DOBDI6Oa7PiLWLY+VeeV7vjgaauqj+P75WkT8AXgaqE1zRK0CMtM//1bJP2AOsEs5vDHFFej/Bt4IPAW8ExgCfAG4E1gdeBNwH7BRudwY4HXl8FTgFw3jExjcsL79gd+Xw+8o40T5el3gmYa4jWVbGrd8vQZFLcdmDeP+Bnywm/c5BPhjOc96TW6bqcAvgPcC/y5jDC7f05hynhOAi4D1KH7YfgN8vZw2CXgI2AJYs4yVwOvL6ROBLSkuhIwDHgb2rNO2A4Li5ORiYE9gg07T3wosKPeT1SiuOL65nHYd8CNgKDAemAfs1FCmxWXM1YBhwIXAT4BXUNTY/Bn4VDn/2cCXy3mHAm/v4XNr3MZDKPbboyn23Z2ARcCbyunTy/Jv3xG7yX1jT+Bf5faZ3sX0bj/bcvqB5f6yBvBdYHbDtOllGd9RTv9ex+fe+f01WdaJwNxyeDXg/eW2fxPwVeDGcnuPAm4A/ruc9+vASeU2HALs0LC/Ld2/ytefotj316RIErcG1mriO2cqRY3mbuVyXwdu7OWzvZLieBtWjtsQuL+hbI2f/3RgPsV+Ohg4Ezint2Oo3E6zgK+U+81rKb4D3lXO+0fgY+XwcGC7Zo7bhvJdTFGj+xqK42JSOe3TwK3A6LI8V3WO1xBnEsWFl66mnQGcXQ7/Dvhkw7RvASeVw++jOD42K7fPfwE39LS9O62nu/e7GPhk+Zl+BnigYTt3e5x39x3c8L31LeDe8vWHGj6ryRS/Vxv29n1BE8cPxUWHx8r3/t1yXE/f9esDH6TY/0cAvwT+tyHeDOBeit+CwcCQZo9f//zr7a/fC+Cff1X9UZwwPAk8AdxDcVI5DPh/wHkN861GcSIwEXg98AiwS+cvW5YvSYnyi/sd5etPAr/rVLYuT7TLcT8GvlYOb0FxdXiNbt7nSeXfFyhOPtYrx/8P8O1ulml8L3+i+LFdmqSU5X+KMkEr53sbRW0NwGkdP2Ll69f39ANJcbJ6Qg233WjgB8BdwBKK5OMN5bSfdJS50zIbAy8CIxrGfZ3yhL4s03UN0zYAnqPhRAjYB7imHP4ZcDIwuol9uvEkdQeKRHG1hulnA1PL4enAz1bguHlFuR/9Dfg7RXOjQT3Mv/Sz7WLaOmWZ124o0zkN04eX23Ljzu+vybJOLD+3JyhOvGYDHy6n3QXs1jDvu4A55fBXgV93tS5enqQcSJHgjGuiPJ33zasapm0OPNPLZ7tTp3GfAE7t5vOfDvy0YdpuwO29HUPAtpQnww3Lfgk4vRy+jqIWY2SnecbQXJLSeNJ8HjClHP4dDSfsFN+x3SUpHwUe6mY7TQOuLIcPanhfQZGYdbzny4BPNCy3GsWV/k26295Nvt87G16vWc7zKno5zruIP5WiJvsJit+c3wFbdzPvbOB95XC33xc0efxQJERzy/L3+F3fxbLjgccbXs8AvtrsMeuff8vzZ3Mvrer2zMx1MnOTzDwkMzuuJt7TMUNmLqH4cXt1Zt4JHEHxA/JIRJwTERst70ozM4FzKH6kAD5CcaWzWWdQ9AcI4GMUSdVznWeKiFdQnMgcm5nfpLg6dlXZBGJ7ih++3vwXxZW5oQ3jRlH8gM0qm208QdE8bFQ5fSOKbdahcZiI2DYirimbCCyguIo6somytGzbleuam5mHZubrKNq7P0VxEgBFMnJXF4ttBDyWmYsaxt1DUdPSoXF7bEJxtf7Bhm35E4orrVAklwH8uWwSdGCT73Mj4L5y/22mHE3JzKcors4fVZbrG8CMjmY5PX22ZZOeaWVTlYUUJ+2w7Ge/tEyZ+SRFcrHcx1iDB8pjfL3MHJ+Z55TjlznOy+GO9XyL4ir7FRHx74iY0kP8nwOXA+dE0Wzsmx3NiZrwUMPw08DQrpo3Nej8eS1t6tVk/OHQ6zG0CbBRx75Y7o9HU5xkQ/F98kbg9iiatu3Rw/qbLhO9fGd08igwsptttWE5HeAC4G0RsSFF7dES4Ppy2ibA9xre42MU+/NKHR80vL/MfLocHE7vx3lXziv33Vdm5k6ZOQsgIvaLoplpR5yxvHQMrej3RaNbKBL2p+nluz4i1oyIn0TEPeUxfR2wTqdmbCuyHaVemaRoIHqA4gcFKDr7UZyQ3g+QmWdl5tvLeZLiJK2zbGI9ZwN7le2gt6X4Qe3Ky2Jl5o0UV9l2oDjB+Hk3y65G0exgSLncFOAvFM1c1qO4mtijzLyS4oTtkIbRj1I0D9mi/BFdJzPXzqKTJ8CDFLUQHTbuFPYsiuYDG2fm2hQ1PR0dQuuy7Tovdx/wQ8p+SxQ/vK/rYtYHgPUiYkTDuNdQ7j9dlOs+iiusIxu25VqZuUW53ocy85OZuRFF06IfRXO34X0A2DiW7ajaUzmalpkvZubvgL9S3HBiHMUVVOj5s/0IRTObXSg6O48pxzd2Bl66r0Rx04f1yvfS15Y5zim2zQMAmbkoM4/KzNdSNHn8fJR9z+i0zbLoD3FsZm4O/AewB7BfBeVdZt1lIvR/KC48rIjujqH7KK6Sr9PwNyIzdwPIzDsycx+Kk+tvAOeXF0NWaF9q0Nt3RqM/UhwzH2gcWe4v7wauLsv6OHAFRZOoj1DU0nWU8z6KmpvG9zksM29oCNnTe1re99vjcd6s8vM6heIGKOtn5jrAzZTH0Ep8X3Snt+/6oyiaT26bmWtRJIOw7DG9svuG1CWTFA1E5wG7R8TO5YnAURQ/LjdExJsiYqcoOtg+S/HlvaSLGPPK8d12EsziFrGPAj8FLs/MJ7qZ9WFgTLz8rig/o2iKtDgbOnN3WsciiqteP4qIDaK4683vynItpGjC1YwvU1yh64i7hOKH8oSIeCVARLw6It5VznIecEAUHbfXpGhC12gERW3DsxHxVooTiA612HZlh9Bjy46oq0XRkf5AigQP4NTyPe5cTn91RLy5TGZuAL4eRafqcRRXn7u8FXJmPkhxIvXtiFirjPW6iPg/ZTk+1NAR9XGKH/yu9rnO/kRxpfoLETEkiudzvIfiCvoKi4gJEbFtw6jXUbSZn1e+7umzHUFxLM2nuDp7XBer2C0i3l7uq/9N0U+jiiuxZwP/FRGjys/2K5SfUUTsUX7uQdFv50Ve2uYP07BvRsSOEbFleeV4IUWfhGY+n5X1duCmzFyhu831cAz9GVgUxQ1ChpW1X2MjYhtYejvzUeV3QMcyS2jiuO3FecDh5XG0DsWd2Lor+wKKJmffj4hJ5f49powxl2UvPJxFkTTuVQ53OAn4UpQ3boiiU/uHlqO8y/V+ezvOl0NHQjivLPcBvHThpLfvi2X23SbL3dt3/QiK38EnoqihP2Y534+0wkxSNOBk5j8p2jx/n+JH/D0Utyp+nuJkbFo5/iGKq4lf6iLG08DXgD+UVeTbdbO6syiuKp/VzXQoOiICzI+IvzaM/znFj1NvzwH5KMWP09/Lch/AS52lT+tlWQAy8w8UJy+NvkhRw3JjWc1/FcUVNTLzMuBE4JqOecplOppVHQJ8NSIWUZwcntewrrpsu+cprvRfRXHyeXNZ/v3Lcv6ZYlueQHEiey0vXZnfp1z2AYrOssdk5lU9rGs/ik7Kt1KcWJxP0WwFYBvgT1Hcvesi4PBs4lkY5f76Hoory49S9LnaLzNv723ZXrwITIuI+yhO/C4DjszMjqZT3X62FMnhPRS1Obfy0n7R6CyKE53HKDqhd3l3uT7wP8BM4CbgHxS1Qv9TTnsDxef+JMVV+x9l5jXltK9TJDdPRMT/pehvcD7FPnIbxX7QVO3cSuqLWw+/7BjKzBcpaoPGA3fzUiKzdjnLJOCWcn/8HkUfn2eW47jtzikUJ/E3UfR3upSic/yLXc1cNl89muK2uQspkvL7gJ07Nd+8iOLzfCgz/96w/IUUNUHnlN9fN1McK01Zwffb03He7HpvBb5NsV8+THGTij80zNLT98VU4IyyvHvTvG6/6yn6nA2j2E9upLgoJrVExx0pJNVMFLeAfAR4S2Z2dYvM2ojiFs43U3RQf6EG5WmbbVdnETE9M/fvy3gUd+Na5Z5X1Nci4lZgr/KkdZUTxa17T8rMTXqdWdKAZE2KVF+fAf5S15PsiHh/FM+iWJfiiuVv6pCglGq97aSelE3hfrYqJShl07LdImJwRLyaojbtwv4ul6T6siZFqqGImEPRMXHPsm157UTEbyluVfkiRROYQ8p22f2qHbbdQGVNysBV9l27FngzRR+HSyiaKq1QnxtJqz6TFEmSJEm1YnMvSZIkSbVSWZISERtH8cCvW6N44NDh5fipEXF/FA8qmh0Ru1VVBkmSJEntp7LmXlE8AXbDzPxrFA89mwXsCewNPJmZxzcba+TIkTlmzJhKyilJkiSp9WbNmvVoZo7qalqzD3pbbmUH2gfL4UURcRvw6hWJNWbMGGbOnNmXxZMkSZLUjyLinu6mtaRPSvmk2K0oHsYEcGhE3BQRp5W3L5UkSZIkoAVJSkQMBy4AjihvNfhj4HUUT7t9kOLJql0td3BEzIyImfPmzau6mJIkSZJqotIkJSKGUCQoZ2bmrwAy8+HMfDEzlwCnAG/tatnMPDkzJ2TmhFGjumyqJkmSJGkVVFmflIgI4FTgtsz8TsP4DRse+PZ+4OaqyiBJkiQ1Y/HixcydO5dnn322v4uyyhk6dCijR49myJAhTS9TWZICbA98DPhHRMwuxx0N7BMR44EE5gCfqrAMkiRJUq/mzp3LiBEjGDNmDMW1dvWFzGT+/PnMnTuXTTfdtOnlqry71++Brj7hS6tapyRJkrQinn32WROUCkQE66+/Psvbx9wnzkuSJElgglKRFdmuJimSJElSDQwaNIjx48czduxYPvShD/H0008v1/IPPPAAe+21FwCzZ8/m0ktfasB00UUXMW3atB6X/8pXvsJVV10FwHe/+93lXn9fquyJ831pwoQJ6cMcJUmSVJXbbruNzTbbbOnrMVMu6dP4c6bt3us8w4cP58knnwRg3333Zeutt+bzn//8Cq1v+vTpzJw5kx/84AcrtHzHw9RHjhy5Qst31nn7AkTErMyc0NX81qRIkiRJNbPDDjtw55138thjj7Hnnnsybtw4tttuO2666SYArr32WsaPH8/48ePZaqutWLRoEXPmzGHs2LE8//zzfOUrX+Hcc89l/PjxnHvuuUyfPp1DDz2UBQsWsMkmm7BkyRIAnnrqKTbeeGMWL17M/vvvz/nnn8+JJ57IAw88wI477siOO+7IaaedxhFHHLG0bKeccgpHHnlkpe/fJEWSJEmqkRdeeIHLLruMLbfckmOOOYatttqKm266ieOOO4799tsPgOOPP54f/vCHzJ49m+uvv55hw4YtXX711Vfnq1/9KpMnT2b27NlMnjx56bS1116b8ePHc+211wJw8cUX8653vWuZ2wMfdthhbLTRRlxzzTVcc8017L333vzmN79h8eLFAJx++ukceOCBlW6DKm9BLEmStErpqQlQM815pJ4888wzjB8/HihqUj7xiU+w7bbbcsEFFwCw0047MX/+fBYuXMj222/P5z//efbdd18+8IEPMHr06KbXM3nyZM4991x23HFHzjnnHA455JAe5x8+fDg77bQTF198MZttthmLFy9myy23XPE32gSTFEmSJKkGhg0bxuzZs3ufEZgyZQq77747l156Kdtvvz2XX345Q4cObWrZ9773vRx99NE89thjzJo1i5122qnXZQ466CCOO+443vzmN3PAAQc0tZ6VYXMvSZIkqaZ22GEHzjzzTABmzJjByJEjWWuttbjrrrvYcsst+eIXv8g222zD7bffvsxyI0aMYNGiRV3GHD58ONtssw2HH344e+yxB4MGDXrZPJ2X33bbbbnvvvs466yz2GefffrwHXbNJEWSJEmqqalTpzJr1izGjRvHlClTOOOMM4DiFsFjx45l3LhxDBkyhHe/+93LLLfjjjty6623Lu0439nkyZP5xS9+sUx/lUYHH3wwkyZNYscdd1w6bu+992b77bdn3XXX7cN32DVvQSxJktQk+6Ssurq6Ra6Wtccee3DkkUey8847L/ey3oJYkiRJUp954okneOMb38iwYcNWKEFZEXaclyRJktStddZZh3/9618tXac1KZIkSZJqxSRFkiRJUq2YpEiSJEmqFZMUSZIkSbVikiJJkiTVQERw1FFHLX19/PHHM3Xq1D5fz3HHHbfM6//4j//ocf6ZM2dy2GGHAcUDJW+44YY+L1Nn3t1LkiRJ6mzq2n0cb0Gvs6yxxhr86le/4ktf+hIjR47s2/U3OO644zj66KOXvu4t6ZgwYQITJhSPM5kxYwbDhw/vNbFZWdakSJIkSTUwePBgDj74YE444YSXTZs3bx4f/OAH2Wabbdhmm234wx/+sHT8O9/5TrbYYgsOOuggNtlkEx599FEA9txzT7beemu22GILTj75ZACmTJnCM888w/jx49l3330BGD58OAAf/vCHueSSlx5Yuv/++3P++eczY8YM9thjD+bMmcNJJ53ECSecwPjx47n++uvZdNNNWbx4MQALFy5c5vXKMEmRJEmSauKzn/0sZ555JgsWLFvzcvjhh3PkkUfyl7/8hQsuuICDDjoIgGOPPZaddtqJW265hb322ot777136TKnnXYas2bNYubMmZx44onMnz+fadOmMWzYMGbPns2ZZ565zDomT57MeeedB8Dzzz/P1Vdfze677750+pgxY/j0pz/NkUceyezZs9lhhx2YOHHi0sTmnHPO4QMf+ABDhgxZ6e1gcy9JkiSpJtZaay32228/TjzxRIYNG7Z0/FVXXcWtt9669PXChQt58skn+f3vf8+FF14IwKRJk1h33XWXznPiiScunXbfffdxxx13sP7663e77ne/+90cfvjhPPfcc/z2t7/lHe94xzJl6MpBBx3EN7/5Tfbcc09OP/10TjnllBV6352ZpEiSJEk1csQRR/CWt7yFAw44YOm4JUuWcOONNzJ06NCmYsyYMYOrrrqKP/7xj6y55ppMnDiRZ599tsdlhg4dysSJE7n88ss599xz+fCHP9zrerbffnvmzJnDjBkzePHFFxk7dmxT5euNzb0kSZKkGllvvfXYe++9OfXUU5eO23XXXfn+97+/9PXs2bOBIknoaKJ1xRVX8PjjjwOwYMEC1l13XdZcc01uv/12brzxxqXLDhkypNt+I5MnT+b000/n+uuvZ9KkSS+bPmLECBYtWrTMuP3224+PfOQjyyRVK8skRZIkSaqZo446amkHeCiabs2cOZNx48ax+eabc9JJJwFwzDHHcMUVVzB27Fh++ctf8qpXvYoRI0YwadIkXnjhBTbbbDOmTJnCdttttzTWwQcfzLhx45Z2nG+06667cu2117LLLruw+uqrv2z6e97zHi688MKlHecB9t13Xx5//HH22WefPnv/kZl9FqwqEyZMyJkzZ/Z3MSRJ0gA3Zsol3U6bM233bqep/m677TY222yz/i7GcnvuuecYNGgQgwcP5o9//COf+cxnltaytMr555/Pr3/9a37+8593O09X2zciZmXmhK7mt0+KJEmS1Kbuvfde9t57b5YsWcLqq6/eZx3Xm/W5z32Oyy67jEsvvbRP45qkSJIkSW3qDW94A3/729/6bf2N/WT6kn1SJEmSJNWKSYokSZIEtENf7Xa0ItvVJEWSJEkD3tChQ5k/f76JSh/LTObPn9/081062CdFkiRJA97o0aOZO3cu8+bN6++irHKGDh3K6NGjl2sZkxRJkiQNeEOGDGHTTTft72KoZHMvSZIkSbVikiJJkiSpVkxSJEmSJNWKSYokSZKkWjFJkSRJklQrJimSJEmSasUkRZIkSVKtmKRIkiRJqhWTFEmSJEm1YpIiSZIkqVZMUiRJkiTVikmKJEmSpFoxSZEkSZJUK5UlKRGxcURcExG3RsQtEXF4OX69iLgyIu4o/69bVRkkSZIktZ8qa1JeAI7KzM2B7YDPRsTmwBTg6sx8A3B1+VqSJEmSgAqTlMx8MDP/Wg4vAm4DXg28DzijnO0MYM+qyiBJkiSp/bSkT0pEjAG2Av4EbJCZD5aTHgI26GaZgyNiZkTMnDdvXiuKKUmSJKkGKk9SImI4cAFwRGYubJyWmQlkV8tl5smZOSEzJ4waNarqYkqSJEmqiUqTlIgYQpGgnJmZvypHPxwRG5bTNwQeqbIMkiRJktpLlXf3CuBU4LbM/E7DpIuAj5fDHwd+XVUZJEmSJLWfwRXG3h74GPCPiJhdjjsamAacFxGfAO4B9q6wDJIkSZLaTGVJSmb+HohuJu9c1XolSZIktTefOC9JkiSpVkxSJEmSJNWKSYokSZKkWjFJkSRJklQrJimSJEmSasUkRZIkSVKtmKRIkiRJqhWTFEmSJEm1YpIiSZIkqVZMUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtWKSIkmSJKlWTFIkSZIk1YpJiiRJkqRaMUmRJEmSVCsmKZIkSZJqxSRFkiRJUq2YpEiSJEmqFZMUSZIkSbVikiJJkiSpVkxSJEmSJNWKSYokSZKkWjFJkSRJklQrJimSJEmSasUkRZIkSVKtmKRIkiRJqhWTFEmSJEm1YpIiSZIkqVZMUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtWKSIkmSJKlWTFIkSZIk1YpJiiRJkqRaMUmRJEmSVCuVJSkRcVpEPBIRNzeMmxoR90fE7PJvt6rWL0mSJKk9VVmTMh2Y1MX4EzJzfPl3aYXrlyRJktSGKktSMvM64LGq4kuSJElaNfVHn5RDI+KmsjnYuv2wfkmSJEk11uok5cfA64DxwIPAt7ubMSIOjoiZETFz3rx5rSqfJEmSpH7W0iQlMx/OzBczcwlwCvDWHuY9OTMnZOaEUaNGta6QkiRJkvpVS5OUiNiw4eX7gZu7m1eSJEnSwDS4qsARcTYwERgZEXOBY4CJETEeSGAO8Kmq1i9JkiSpPVWWpGTmPl2MPrWq9UmSJElaNfjEeUmSJEm1YpIiSZIkqVZMUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmqlslsQS5IkSXU3Zsol3U6bM233FpZEjZqqSYmI10XEGuXwxIg4LCLWqbZokiRJkgaiZpt7XR2IxDUAABIvSURBVAC8GBGvB04GNgbOqqxUkiRJkgasZpOUJZn5AvB+4PuZ+Z/AhtUVS5IkSdJA1WySsjgi9gE+DlxcjhtSTZEkSZIkDWTNJikHAG8DvpaZd0fEpsDPqyuWJEmSpIGqqbt7ZeatEfFF4DXl67uBb1RZMEmSJEkDU7N393oPMBv4bfl6fERcVGXBJEmSJA1MzTb3mgq8FXgCIDNnA6+tqEySJEmSBrCmO85n5oJO45b0dWEkSZIkqdknzt8SER8BBkXEG4DDgBuqK5YkSZKkgarZmpTPAVsAz1E8xHEBcERVhZIkSZI0cPVakxIRg4BLMnNH4MvVF0mSJEnSQNZrTUpmvggsiYi1W1AeSZIkSQNcs31SngT+ERFXAk91jMzMwyoplSRJkqQBq9kk5VflnyRJkiRVqtknzp8REasDbyxH/TMzF1dXLEmSJEkDVVNJSkRMBM4A5gABbBwRH8/M66ormiRJkqSBqNnmXt8Gds3MfwJExBuBs4GtqyqYJEmSpIGp2eekDOlIUAAy81/AkGqKJEmSJGkga7YmZWZE/BT4Rfl6X2BmNUWSJEmSNJA1m6R8Bvgs0HHL4euBH1VSIkmSJEkDWrNJymDge5n5HVj6FPo1KiuVJEmSpAGr2T4pVwPDGl4PA67q++JIkiRJGuiaTVKGZuaTHS/K4TWrKZIkSZKkgazZJOWpiHhLx4uImAA8U02RJEmSJA1kzfZJOQL4ZUQ8UL7eEJhcTZEkSZIkDWQ91qRExDYR8arM/AvwZuBcYDHwW+DuFpRPkiRJ0gDTW3OvnwDPl8NvA44Gfgg8DpxcYbkkSZIkDVC9NfcalJmPlcOTgZMz8wLggoiYXW3RJEmSJA1EvdWkDIqIjkRmZ+B3DdOa7c8iSZIkSU3rLdE4G7g2Ih6luJvX9QAR8XpgQcVlkyRJkjQA9ZikZObXIuJqirt5XZGZWU5aDfhc1YWTJEmSNPD02mQrM2/sYty/qimOJEmSpIGu2Yc5SpIkSVJL2PldkiS11Jgpl3Q7bc603VtYEkl1ZU2KJEmSpFoxSZEkSZJUKyYpkiRJkmqlsiQlIk6LiEci4uaGcetFxJURcUf5f92q1i9JkiSpPVVZkzIdmNRp3BTg6sx8A3B1+VqSJEmSlqosScnM64DHOo1+H3BGOXwGsGdV65ckSZLUnlrdJ2WDzHywHH4I2KDF65ckSZJUc/3WcT4zE8jupkfEwRExMyJmzps3r4UlkyRJktSfWp2kPBwRGwKU/x/pbsbMPDkzJ2TmhFGjRrWsgJIkSZL6V6uTlIuAj5fDHwd+3eL1S5IkSaq5wVUFjoizgYnAyIiYCxwDTAPOi4hPAPcAe1e1fkmSpJaaunYP0xa0rhzSKqCyJCUz9+lm0s5VrVOSJElS+6ssSZEkSdUZM+WSbqfNmbZ7C0siSX2v3+7uJUmSJEldMUmRJEmSVCsmKZIkSZJqxSRFkiRJUq2YpEiSJEmqFZMUSZIkSbVikiJJkiSpVkxSJEmSJNWKD3OUpAHAB/9JktqJNSmSJEmSasUkRZIkSVKtmKRIkiRJqhWTFEmSJEm1YpIiSZIkqVZMUiRJkiTVircgltRy3g5XkiT1xJoUSZIkSbViTcpy8gqwJEmSVC1rUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtWKSIkmSJKlWTFIkSZIk1YpJiiRJkqRaMUmRJEmSVCsmKZIkSZJqxSRFkiRJUq2YpEiSJEmqFZMUSZIkSbUyuL8LIPWnMVMu6XbanGm7t7AkkiRJ6mBNiiRJkqRaMUmRJEmSVCs295IkSZK6MnXtHqYtaF05BiBrUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtdIvz0mJiDnAIuBF4IXMnNAf5ZAkSZJUP/35MMcdM/PRfly/JEmSpBqyuZckSZKkWumvJCWBKyJiVkQc3E9lkCRJklRD/dXc6+2ZeX9EvBK4MiJuz8zrGmcok5eDAV7zmtf0Rxkl9Yepa/cwbUHryiFJkvpNv9SkZOb95f9HgAuBt3Yxz8mZOSEzJ4waNarVRZQkSZLUT1qepETEKyJiRMcwsCtwc6vLIUmSJKme+qO51wbAhRHRsf6zMvO3/VAOSZIkSTXU8iQlM/8N/H+tXq8kSZKk9uAtiCVJkiTVSn8+zFGSJNXUmCmXdDttzrTdW1gSDXTuiwOTNSmSJEmSasWaFEmSKuIVYElaMdakSJIkSaoVkxRJkiRJtWKSIkmSJKlWTFIkSZIk1YpJiiRJkqRaMUmRJEmSVCvegliSBrqpa/cwbUHryqG+42cqqc1ZkyJJkiSpVkxSJEmSJNWKzb0kSVJ92FRNEiYpUvf8oZQkSeoXNveSJEmSVCvWpEiSam3MlEu6nTZn2u4tLIkkqVWsSZEkSZJUK9akSFqleNVdkqT2Z5Ki2vOkU5IkaWCxuZckSZKkWrEmRZIkaRVnqwS1G2tSJEmSJNWKSYokSZKkWrG5V1/yCeWSJEmt47nXKsskpUZsLypJkiSZpEiSJEmV8AL0ijNJkSrkl5MkSdLys+O8JEmSpFqxJkWSJC0fOytLqphJiiRJktRmVvUm5SYpUpta1b+cpFbwOJKkerJPiiRJkqRasSZFkiRJajX7dvXIJEXtzQNcUrvy+0uSumVzL0mSJEm1Yk3KAGHnUEmSJLULkxRJUvuyyZQ68aKctGqwuZckSZKkWrEmRV6JlCRpIPM8QDVkkqI+YfW62kLNf4g9jqSK1fw7QNJLTFLahV+sqx4/U0mSpC7ZJ0WSJElSrViTIknLoccmWUM/0v2CK1s7Zs1b67nNJanfmKSoev7Qt52eTsTB/hGSVAX7panPrALnXv2SpETEJOB7wCDgp5k5rT/KIa2yVoEvJ7UXT66kivm9rgGm5X1SImIQ8EPg3cDmwD4RsXmryyFJkiSpnvqjJuWtwJ2Z+W+AiDgHeB9waz+URdKK8IqeJEmqUH8kKa8G7mt4PRfYth/KIUlqBZNaaZXXbzcV0SorMrO1K4zYC5iUmQeVrz8GbJuZh3aa72Dg4PLlm4B/trSgK2Yk8KixWxq/XWNXHd+ytz521fHbNXbV8ds1dtXxLXvrY1cdv11jVx3fsrc+dl/aJDNHdTWhP2pS7gc2bng9uhy3jMw8GTi5VYXqCxExMzMnGLt18ds1dtXxLXvrY1cdv11jVx2/XWNXHd+ytz521fHbNXbV8S1762O3Sn88zPEvwBsiYtOIWB34MHBRP5RDkiRJUg21vCYlM1+IiEOByyluQXxaZt7S6nJIkiRJqqd+eU5KZl4KXNof665Ylc3T2jV21fHbNXbV8S1762NXHb9dY1cdv11jVx3fsrc+dtXx2zV21fEte+tjt0TLO85LkiRJUk/6o0+KJEmSJHXLJGUlRcRpEfFIRNzcbvErjr1xRFwTEbdGxC0RcXi7xG/X2FXHb9fYrYhfrmNQRPwtIi5up9hVxo+IORHxj4iYHREz+zJ21fEjYp2IOD8ibo+I2yLibe0Qu8r4EfGmclt3/C2MiCP6InaL4h9ZHv83R8TZETG0HWJXHT8iDi/j3tIX27urc4uIWC8iroyIO8r/6w6k2D3E/1C53ZdExArfiaub2N8qvwNuiogLI2KdFY3fbzLTv5X4A94BvAW4ud3iVxx7Q+At5fAI4F/A5u0Qv11jt3PZ23m7NKzj88BZwMV9Gbfq2FXGB+YAI6soc9XxgTOAg8rh1YF12iF2K+KXcQcBD1E846CK7d+n8SkeJH03MKx8fR6wf91jt6DsY4GbgTUp+ilfBbx+JWO+7NwC+CYwpRyeAnxjIMXuIf5mFM8CnAFM6OPYuwKDy+FvrEzZ++vPmpSVlJnXAY+1Y/yKYz+YmX8thxcBt1F80dY+frvGrjp+u8ZuRfyIGA3sDvy0r2K2InYr4rejiFib4kf/VIDMfD4zn6h77FbEb7AzcFdm3lNB7KriDwaGRcRgipPyB9okdpXxNwP+lJlPZ+YLwLXAB1YmYDfnFu+jSJ4p/+85kGJ3Fz8zb8vMlX5YeTexryg/U4AbKZ5L2FZMUlS5iBgDbAX8qd3it2vsquO3a+wK438X+AKwpA9jtiJ21fETuCIiZkXEwW0Uf1NgHnB62QzupxHxijaI3Yr4HT4MnF1B3EriZ+b9wPHAvcCDwILMvKLusVsQ/2Zgh4hYPyLWBHZj2Qdu95UNMvPBcvghYANjt9SBwGX9XYjlZZKiSkXEcOAC4IjMXNhO8ds1dtXx2zV2VfEjYg/gkcyc1RfxWhW7FfGBt2fmW4B3A5+NiHe0SfzBFE0nfpyZWwFPUTT1qHvsVsQnigcxvxf4ZV/GrTJ+2ZfgfRRJ3EbAKyLio3WPXXX8zLyNoinQFcBvgdnAi30Ru4d1JsUFBmO3QER8GXgBOLO/y7K8TFJUmYgYQnFCeGZm/qqd4rdr7Krjt2vsiuNvD7w3IuYA5wA7RcQv2iB25fHLK8Bk5iPAhcBb+yp2xfHnAnMzs6O27XyKE/+6x25FfCiSwr9m5sN9HLfK+LsAd2fmvMxcDPwK+I82iF15/Mw8NTO3zsx3AI9T9Nnraw9HxIYA5f9HjF29iNgf2APYt0yy2opJiioREUHRJvq2zPxOO8Vv19hVx2/X2FXHz8wvZebozBxD0UTld5nZV1c5K4tddfyIeEVEjOgYpujE2Wd3EqwyfmY+BNwXEW8qR+0M3Fr32K2IX9qHapt6VRH/XmC7iFiz/D7YmaJvWt1jVx4/Il5Z/n8NRX+Us/oqdoOLgI+Xwx8Hfm3sakXEJIqmvO/NzKf7uzwrJGvQe7+d/yi+SB8EFlNcwfpEu8SvOPbbKapFb6KoPp4N7NYO8ds1djuXvZ23S6f1TKS6O3BVFruK+MBrgb+Xf7cAX+7j8lYdfzwws9xn/hdYtx1it6DsrwDmA2v3ZZlbER84FridIpn9ObBGO8RuQdmvp0hk/w7s3AfxXnZuAawPXA3cQXEHsfUGUuwe4r+/HH4OeBi4vA9j3wnc1/Cbd1Jf7pOt+POJ85IkSZJqxeZekiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtWKSIkmSJKlWTFIkaYCLiGx8iGNEDI6IeRFx8QrGWyciDml4PbGZWBHx1YjYZTnW886ImBUR/yj/79Qwbety/J0RcWL5fAki4lsRcXtE3BQRF0bEOuX49SPimoh4MiJ+sHzvWJLU10xSJElPAWMjYlj5+p3A/SsRbx3gkF7n6iQzv5KZVy3HIo8C78nMLSketPbzhmk/Bj4JvKH8m1SOvxIYm5njKJ6s/aVy/LPA/wP+7/KWW5LU90xSJEkAlwK7l8PLPPE7ItaLiP8tax9ujIhx5fipEXFaRMyIiH9HxGHlItOA10XE7Ij4VjlueEScX9ZinNlRs9EoIqZHxF7l8JyIODYi/lrWiLy58/yZ+bfMfKB8eQswLCLWiIgNgbUy88YsHgb2M2DPcpkrMvOFcpkbgdHl+Kcy8/cUyYokqZ+ZpEiSAM4BPhwRQ4FxwJ8aph0L/K2sfTia4qS/w5uBdwFvBY6JiCHAFOCuzByfmf9ZzrcVcASwOcXT4rdvokyPZuZbKGpFeqvh+CDw18x8Dng1xVOXO8wtx3V2IHBZE+WQJLWYSYokicy8CRhDUYtyaafJb6dsSpWZvwPWj4i1ymmXZOZzmfko8AiwQTer+HNmzs3MJcDscl29+VX5f1ZP80fEFsA3gE81EbNjmS8DLwBnNruMJKl1Bvd3ASRJtXERcDwwEVi/yWWeaxh+ke5/V5qdr6tlup0/IkYDFwL7ZeZd5ej7KZtxlUbT0McmIvYH9gB2LpuDSZJqxpoUSVKH04BjM/MfncZfD+wLxZ26KJphLewhziJgRCUlbFDemesSYEpm/qFjfGY+CCyMiO3Kvi/7Ab8ul5kEfAF4b2Y+XXUZJUkrxiRFkgRA2RzrxC4mTQW2joibKDrFf7yXOPOBP0TEzQ0d56twKPB64CtlJ/3ZEfHKctohwE+BO4G7eKnvyQ8oEqgry/lP6ggWEXOA7wD7R8TciNi8wrJLknoQ1nRLkiRJqhNrUiRJkiTVikmKJEmSpFoxSZEkSZJUKyYpkiRJkmrFJEWSJElSrZikSJIkSaoVkxRJkiRJtWKSIkmSJKlW/n/gldwwY5ZKwgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 2160x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post URLs in Same Chronological Order as Chart Above:\n",
            "Month:  1 URL:  https://www.reddit.com/r/investing/comments/l3z5jq/i_created_an_algo_that_tracks_the_most_hyped/\n",
            "Month:  1 URL:  https://www.reddit.com/r/investing/comments/l7jppw/what_stocks_are_you_looking_to_grab_at_a_possible/\n",
            "Month:  1 URL:  https://www.reddit.com/r/investing/comments/l6eyjw/apple_reports_blowout_quarter_booking_more_than/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lrgpra/from_investorscom_6_top_warren_buffett_stocks/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lhwg4r/surge_by_disney_to_nearly_95_million/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lqqoks/watch_your_ps_ratio_ps_over_30_is_incredibly_risky/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lg297f/second_quarter_results_6_months_into_the/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/ljpowg/analysing_institutional_investor_transactions/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/luoy2j/all_pes_are_not_created_equal_lowes_vs_home_depot/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lf0qz5/hyundai_now_says_its_not_in_talks_with_apple_to/\n",
            "Month:  2 URL:  https://www.reddit.com/r/investing/comments/lcftzc/10_interesting_and_useful_etfs_with_less_than_1b/\n",
            "Month:  4 URL:  https://www.reddit.com/r/investing/comments/mijk9h/i_computed_the_weighted_average_upside_for_100/\n",
            "Month:  4 URL:  https://www.reddit.com/r/investing/comments/mqje1g/apple_to_hold_first_product_unveiling_of_the_year/\n",
            "Month:  4 URL:  https://www.reddit.com/r/investing/comments/n0ojht/apple_reports_another_blowout_quarter_with_sales/\n",
            "Month:  5 URL:  https://www.reddit.com/r/investing/comments/n74k0c/using_math_followed_by_some_due_diligence_to/\n",
            "Month:  5 URL:  https://www.reddit.com/r/investing/comments/n89kz5/we_sometimes_say_that_we_plan_to_hold_certain/\n",
            "Month:  6 URL:  https://www.reddit.com/r/investing/comments/nprdln/10_more_interesting_and_useful_etfs_with_less/\n",
            "Month:  6 URL:  https://www.reddit.com/r/investing/comments/nu3wrs/should_spotify_be_given_a_spot_in_a_stock/\n",
            "Month:  6 URL:  https://www.reddit.com/r/investing/comments/nrnnv0/i_think_aapl_is_at_its_peak_and_starts_to_show/\n",
            "Month:  7 URL:  https://www.reddit.com/r/investing/comments/osx7or/apple_demolishes_earnings_expectations_but_stock/\n",
            "Month:  7 URL:  https://www.reddit.com/r/investing/comments/oqje9y/when_does_a_company_reach_its_full_potential_when/\n",
            "Month:  8 URL:  https://www.reddit.com/r/investing/comments/pbyofq/tsm_hikes_chip_prices_up_to_20_amid_supply/\n",
            "Month:  8 URL:  https://www.reddit.com/r/investing/comments/p0r3o4/time_to_take_aapl_gains_and_gtfo/\n",
            "Month:  8 URL:  https://www.reddit.com/r/investing/comments/pcz014/affirm_shares_soar_on_news_of_amazon_partnership/\n",
            "Month:  9 URL:  https://www.reddit.com/r/investing/comments/pifa8n/my_top_tech_stocks_to_watch_that_pays_dividends/\n",
            "Month:  10 URL:  https://www.reddit.com/r/investing/comments/q4n6wv/why_do_some_stocks_have_many_gaps_and_look_spaced/\n",
            "Month:  10 URL:  https://www.reddit.com/r/investing/comments/qdua1s/how_are_you_preparing_for_2022/\n",
            "Month:  10 URL:  https://www.reddit.com/r/investing/comments/qfr8l1/tesla_passes_1t_valuation_on_news_of_hertz/\n",
            "Month:  11 URL:  https://www.reddit.com/r/investing/comments/qzp37a/blue_chips_can_be_your_bangers/\n",
            "Month:  12 URL:  https://www.reddit.com/r/investing/comments/knnc52/what_is_your_longterm_view_of_msft/\n",
            "--- 78.41880965232849 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# Search Query can be modified with tools described here:\n",
        "# https://www.reddit.com/wiki/search\n",
        "subreddit = 'investing'\n",
        "query = \"$aapl\"\n",
        "\n",
        "start_time = time.time()\n",
        "posts = fetch_posts(subreddit, query)\n",
        "posts_with_extracted_comments = extract_post_comments(posts)\n",
        "posts_with_pos_neg_scores = find_pos_neg_scores(posts_with_extracted_comments)\n",
        "plot_pos_neg_scores(posts_with_pos_neg_scores, subreddit, query)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reddit_sentiment_v2_GitHub.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}